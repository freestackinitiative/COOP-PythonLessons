{"cells":[{"cell_type":"markdown","id":"51dd7a33","metadata":{"id":"51dd7a33"},"source":["<img src=\"https://tinyurl.com/k2t79s6t\" style=\"float: left; margin: 20px; height: 55px\">\n","\n","# Python 105: Data Analytics Salary Analysis\n"," _Author: Martin Arroyo_\n","\n","\n","\n","## Scenario\n","You are an analyst working on a new data set that your company acquired for research purposes. It contains salary information for data analytics professionals collected between 2020 and 2022. \n","\n","Management has asked you to do an exploratory analysis of this new data. [Click here to read the email they sent you.](https://docs.google.com/document/d/1J-3SG76KQxj-aeKQ6-ZmRuntsAdoPqC8lMtQPzuhVJQ/edit?usp=sharing)\n","\n","\n","## Task\n","\n","### Part One\n","Clean the `ds_salaries` table using the outline provided below. Use documentation to refresh your memory on syntax. After you're done cleaning, your data should have no null values and no duplicates.\n","### Part Two\n","Complete the analysis using everything we've learned so far in Python 101-104. Ensure that the data is clean, then answer the questions in the code cells under **Part Two**. Use a combination of visuals, tabular data, and written language to answer the questions. **Create at least 3 visuals**.\n","### Part Three\n","After you're done, write a response to your manager's email letting them know that you have completed your assignment. Share any additional insights or findings you may have found in the data, along with: \n","\n","- a copy of your notebook with your code and visuals\n","- the `ds_salaries_clean` table as a `csv` file\n","\n","This should be a professional email response. In this scenario, your pod captain will play the role of manager.\n","\n","<hr />\n","\n","\n","## Tips\n","\n","### Using Python + SQL\n","\n","Since we are connected to a database, you can use a combination of `SQL` and `Python` to answer your questions.\n","\n","To query the database directly, you can use the `%%sql` cell magic at the top of a cell, then write your query below like this:\n","```sql\n","%%sql\n","\n","SELECT * FROM ds_salaries;\n","``` \n","\n","Once you have the results you like, copy and paste the query into a `string`, then get your result as a `DataFrame` using `read_sql`:\n","```python\n","query = \"SELECT * FROM ds_salaries\"\n","my_query_df = pd.read_sql(query, con=engine)\n","```\n","*Note: If you're using the `%%sql` cell magic, the connection to the database will sometimes drop, giving an error. This is expected. Just run the same cell again and your query should execute.*\n","\n","### Keep in Mind\n","\n","This is not a client project, nor will this be part of a presentation. It should be neat, but any visuals you make do not need to be fancy. \n","\n","### Best Practices\n","\n","If you get stuck, first do a search for the problem you're trying to solve. More often than not, someone else has encountered a similar issue and you'll be able to find a relatively clear answer. You can also check the documentation of the program you're using. If you find yourself getting frustrated, stop and take a short break. If all else fails, then reach out to another person for help.\n","\n","### Troubleshooting\n","\n","There may be things here that you don't remember how to do, or maybe there is something you want to do that we haven't covered. Use the resources provided to you in Slack and from the other Python nights, the Python 101-104 notebooks, as well as whatever you can find on the internet to help you. \n","\n","Here are a few to get you started:\n","- [w3schools](https://www.w3schools.com/python/)\n","- [DataCamp Cheat Sheets](https://www.datacamp.com/cheat-sheet)\n","- [Stack Overflow](https://stackoverflow.com/)\n","- [SQL Basics Cheatsheet](https://martinmarroyo.github.io/sqlcheatsheetandresources-coop/)"]},{"cell_type":"markdown","id":"726c07a6","metadata":{"id":"726c07a6"},"source":["## Connect to the Database and get the data"]},{"cell_type":"markdown","id":"67e63a28","metadata":{"id":"67e63a28"},"source":["### Anaconda/Local Users ONLY!\n","Uncomment and run the cell below to install the required libraries"]},{"cell_type":"code","execution_count":1,"id":"fb1ebb9e","metadata":{"id":"fb1ebb9e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681577397678,"user_tz":240,"elapsed":16311,"user":{"displayName":"Martin Arroyo","userId":"00023833307036255373"}},"outputId":"22b8e337-ddd6-4143-d2b2-bf84c59ba0ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting psycopg2-binary\n","  Downloading psycopg2_binary-2.9.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sqlalchemy==1.4.47\n","  Downloading SQLAlchemy-1.4.47-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: ipython-sql==0.4.1 in /usr/local/lib/python3.9/dist-packages (0.4.1)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from sqlalchemy==1.4.47) (2.0.2)\n","Requirement already satisfied: prettytable<1 in /usr/local/lib/python3.9/dist-packages (from ipython-sql==0.4.1) (0.7.2)\n","Requirement already satisfied: ipython-genutils>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from ipython-sql==0.4.1) (0.2.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from ipython-sql==0.4.1) (1.16.0)\n","Requirement already satisfied: ipython>=1.0 in /usr/local/lib/python3.9/dist-packages (from ipython-sql==0.4.1) (7.34.0)\n","Requirement already satisfied: sqlparse in /usr/local/lib/python3.9/dist-packages (from ipython-sql==0.4.1) (0.4.3)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.9/dist-packages (from ipython>=1.0->ipython-sql==0.4.1) (2.14.0)\n","Collecting jedi>=0.16\n","  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.9/dist-packages (from ipython>=1.0->ipython-sql==0.4.1) (67.6.1)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.9/dist-packages (from ipython>=1.0->ipython-sql==0.4.1) (5.7.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from ipython>=1.0->ipython-sql==0.4.1) (3.0.38)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython>=1.0->ipython-sql==0.4.1) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython>=1.0->ipython-sql==0.4.1) (0.7.5)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.9/dist-packages (from ipython>=1.0->ipython-sql==0.4.1) (4.8.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.9/dist-packages (from ipython>=1.0->ipython-sql==0.4.1) (0.1.6)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython>=1.0->ipython-sql==0.4.1) (0.2.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.16->ipython>=1.0->ipython-sql==0.4.1) (0.8.3)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect>4.3->ipython>=1.0->ipython-sql==0.4.1) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=1.0->ipython-sql==0.4.1) (0.2.6)\n","Installing collected packages: sqlalchemy, psycopg2-binary, jedi\n","  Attempting uninstall: sqlalchemy\n","    Found existing installation: SQLAlchemy 2.0.9\n","    Uninstalling SQLAlchemy-2.0.9:\n","      Successfully uninstalled SQLAlchemy-2.0.9\n","Successfully installed jedi-0.18.2 psycopg2-binary-2.9.6 sqlalchemy-1.4.47\n"]}],"source":["!pip install psycopg2-binary sqlalchemy==1.4.47 ipython-sql==0.4.1"]},{"cell_type":"markdown","source":["### Everyone"],"metadata":{"id":"DOy6qJxta96w"},"id":"DOy6qJxta96w"},{"cell_type":"markdown","id":"d813af0a","metadata":{"id":"d813af0a"},"source":["Run the following three cells to establish the connection to your team's database and get the data:"]},{"cell_type":"code","execution_count":1,"id":"62a30d37","metadata":{"id":"62a30d37","executionInfo":{"status":"ok","timestamp":1681577591317,"user_tz":240,"elapsed":583,"user":{"displayName":"Martin Arroyo","userId":"00023833307036255373"}}},"outputs":[],"source":["%load_ext sql"]},{"cell_type":"code","execution_count":2,"id":"befffe7d","metadata":{"id":"befffe7d","executionInfo":{"status":"ok","timestamp":1681577592255,"user_tz":240,"elapsed":137,"user":{"displayName":"Martin Arroyo","userId":"00023833307036255373"}}},"outputs":[],"source":["import os\n","import pandas as pd\n","import sqlalchemy as sql\n","import matplotlib.pyplot as plt\n","\n","os.environ[\"DATABASE_URL\"] = \"postgresql://captmarroyo:v2_3uAvV_vFYzpBDytViYWU6RBVLBbJU@db.bit.io/captmarroyo/coopdataanalytics\"\n","# Create engine to connect to database w/ Python\n","engine = sql.create_engine(os.environ[\"DATABASE_URL\"])"]},{"cell_type":"code","execution_count":3,"id":"fb6fc1ae","metadata":{"id":"fb6fc1ae","executionInfo":{"status":"ok","timestamp":1681577597029,"user_tz":240,"elapsed":2464,"user":{"displayName":"Martin Arroyo","userId":"00023833307036255373"}}},"outputs":[],"source":["# Get the data from the database into a DataFrame\n","ds_salaries = pd.read_sql(\"SELECT * FROM ds_salaries\", con=engine)\n","countries = pd.read_sql(\"SELECT * FROM countries\", con=engine)\n","usd_exchange_rates = pd.read_sql(\"SELECT * FROM usd_exchange_rates\", con=engine)\n","experience_levels = pd.read_sql(\"SELECT * FROM experience_levels\", con=engine)\n","employment_types = pd.read_sql(\"SELECT * FROM employment_types\", con=engine)"]},{"cell_type":"markdown","id":"d93c2a97","metadata":{"id":"d93c2a97"},"source":["## Part One: Cleaning the data"]},{"cell_type":"markdown","id":"4440bb70","metadata":{"id":"4440bb70"},"source":["Our task is to explore this new dataset, clean it, then answer the questions from management using the data. We haven't seen the data before, so our first step is familiarize ourselves with it. \n","\n","Since the data has not been cleaned, we consider it to be in its \"raw\" form. It's good practice to not modify raw data directly - that way you can revert back to the original state when/if you need to. Let's start by making a copy of `ds_salaries`:"]},{"cell_type":"code","execution_count":null,"id":"0e35dc94","metadata":{"id":"0e35dc94"},"outputs":[],"source":["ds_salaries_clean = ds_salaries.copy()"]},{"cell_type":"code","source":["ds_salaries_clean"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"Vu-V2eaOOhq_","executionInfo":{"status":"ok","timestamp":1680829205957,"user_tz":240,"elapsed":255,"user":{"displayName":"Carl Gordon","userId":"05008020006991877451"}},"outputId":"5d79c3c1-ff8f-4130-dfcb-bd9469752f3e"},"id":"Vu-V2eaOOhq_","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     work_year experience_level employment_type                   job_title  \\\n","0         2020               MI              FT              Data Scientist   \n","1         2020               SE              FT  Machine Learning Scientist   \n","2         2020               SE              FT           Big Data Engineer   \n","3         2020               MI              FT        Product Data Analyst   \n","4         2020               SE              FT   Machine Learning Engineer   \n","..         ...              ...             ...                         ...   \n","623       2022               MI              FT               Data Engineer   \n","624       2022               SE              FT              Data Scientist   \n","625       2022               SE              FT       Data Science Engineer   \n","626       2022               MI              FT               Data Engineer   \n","627       2022               MI              FT  Machine Learning Scientist   \n","\n","       salary salary_currency employee_residence  remote_ratio  \\\n","0     70000.0             EUR                 DE             0   \n","1    260000.0             USD                 JP             0   \n","2     85000.0             GBP                 GB            50   \n","3     20000.0             USD                 HN             0   \n","4    150000.0             USD                 US            50   \n","..        ...             ...                ...           ...   \n","623   45000.0             GBP                 GB           100   \n","624  260000.0             USD                 US           100   \n","625   60000.0             USD                 AR           100   \n","626   63900.0             USD                 US             0   \n","627  160000.0             USD                 US           100   \n","\n","    remote_work_type company_location company_size      job_category  \n","0       On-site only               DE            L      Data Science  \n","1       On-site only               JP            S  Machine Learning  \n","2             Hybrid               UK            M  Data Engineering  \n","3       On-site only               HN            S      Data Analyst  \n","4             Hybrid               US            L  Machine Learning  \n","..               ...              ...          ...               ...  \n","623     Fully remote               UK            M  Data Engineering  \n","624     Fully remote               US            M      Data Science  \n","625     Fully remote               MX            L  Data Engineering  \n","626     On-site only               US            M  Data Engineering  \n","627     Fully remote               US            L  Machine Learning  \n","\n","[628 rows x 12 columns]"],"text/html":["\n","  <div id=\"df-092a27ac-5f5d-42b3-ab1a-bd2665e2a3dc\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>work_year</th>\n","      <th>experience_level</th>\n","      <th>employment_type</th>\n","      <th>job_title</th>\n","      <th>salary</th>\n","      <th>salary_currency</th>\n","      <th>employee_residence</th>\n","      <th>remote_ratio</th>\n","      <th>remote_work_type</th>\n","      <th>company_location</th>\n","      <th>company_size</th>\n","      <th>job_category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2020</td>\n","      <td>MI</td>\n","      <td>FT</td>\n","      <td>Data Scientist</td>\n","      <td>70000.0</td>\n","      <td>EUR</td>\n","      <td>DE</td>\n","      <td>0</td>\n","      <td>On-site only</td>\n","      <td>DE</td>\n","      <td>L</td>\n","      <td>Data Science</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2020</td>\n","      <td>SE</td>\n","      <td>FT</td>\n","      <td>Machine Learning Scientist</td>\n","      <td>260000.0</td>\n","      <td>USD</td>\n","      <td>JP</td>\n","      <td>0</td>\n","      <td>On-site only</td>\n","      <td>JP</td>\n","      <td>S</td>\n","      <td>Machine Learning</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2020</td>\n","      <td>SE</td>\n","      <td>FT</td>\n","      <td>Big Data Engineer</td>\n","      <td>85000.0</td>\n","      <td>GBP</td>\n","      <td>GB</td>\n","      <td>50</td>\n","      <td>Hybrid</td>\n","      <td>UK</td>\n","      <td>M</td>\n","      <td>Data Engineering</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2020</td>\n","      <td>MI</td>\n","      <td>FT</td>\n","      <td>Product Data Analyst</td>\n","      <td>20000.0</td>\n","      <td>USD</td>\n","      <td>HN</td>\n","      <td>0</td>\n","      <td>On-site only</td>\n","      <td>HN</td>\n","      <td>S</td>\n","      <td>Data Analyst</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2020</td>\n","      <td>SE</td>\n","      <td>FT</td>\n","      <td>Machine Learning Engineer</td>\n","      <td>150000.0</td>\n","      <td>USD</td>\n","      <td>US</td>\n","      <td>50</td>\n","      <td>Hybrid</td>\n","      <td>US</td>\n","      <td>L</td>\n","      <td>Machine Learning</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>623</th>\n","      <td>2022</td>\n","      <td>MI</td>\n","      <td>FT</td>\n","      <td>Data Engineer</td>\n","      <td>45000.0</td>\n","      <td>GBP</td>\n","      <td>GB</td>\n","      <td>100</td>\n","      <td>Fully remote</td>\n","      <td>UK</td>\n","      <td>M</td>\n","      <td>Data Engineering</td>\n","    </tr>\n","    <tr>\n","      <th>624</th>\n","      <td>2022</td>\n","      <td>SE</td>\n","      <td>FT</td>\n","      <td>Data Scientist</td>\n","      <td>260000.0</td>\n","      <td>USD</td>\n","      <td>US</td>\n","      <td>100</td>\n","      <td>Fully remote</td>\n","      <td>US</td>\n","      <td>M</td>\n","      <td>Data Science</td>\n","    </tr>\n","    <tr>\n","      <th>625</th>\n","      <td>2022</td>\n","      <td>SE</td>\n","      <td>FT</td>\n","      <td>Data Science Engineer</td>\n","      <td>60000.0</td>\n","      <td>USD</td>\n","      <td>AR</td>\n","      <td>100</td>\n","      <td>Fully remote</td>\n","      <td>MX</td>\n","      <td>L</td>\n","      <td>Data Engineering</td>\n","    </tr>\n","    <tr>\n","      <th>626</th>\n","      <td>2022</td>\n","      <td>MI</td>\n","      <td>FT</td>\n","      <td>Data Engineer</td>\n","      <td>63900.0</td>\n","      <td>USD</td>\n","      <td>US</td>\n","      <td>0</td>\n","      <td>On-site only</td>\n","      <td>US</td>\n","      <td>M</td>\n","      <td>Data Engineering</td>\n","    </tr>\n","    <tr>\n","      <th>627</th>\n","      <td>2022</td>\n","      <td>MI</td>\n","      <td>FT</td>\n","      <td>Machine Learning Scientist</td>\n","      <td>160000.0</td>\n","      <td>USD</td>\n","      <td>US</td>\n","      <td>100</td>\n","      <td>Fully remote</td>\n","      <td>US</td>\n","      <td>L</td>\n","      <td>Machine Learning</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>628 rows × 12 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-092a27ac-5f5d-42b3-ab1a-bd2665e2a3dc')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-092a27ac-5f5d-42b3-ab1a-bd2665e2a3dc button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-092a27ac-5f5d-42b3-ab1a-bd2665e2a3dc');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["Now that we have our copy, let's begin exploring the data and its structure. Use the summary functions that we learned in `Python 103` to look at the metadata for `ds_salaries_clean` and observe the number of rows, columns, datatypes, and null values:\n"],"metadata":{"id":"xJJfEnBVCP7r"},"id":"xJJfEnBVCP7r"},{"cell_type":"code","source":[],"metadata":{"id":"JV8epcNRDOhE"},"id":"JV8epcNRDOhE","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check the first 5 rows of the data using the `head` method:"],"metadata":{"id":"YF52QbDl5_j3"},"id":"YF52QbDl5_j3"},{"cell_type":"code","source":[],"metadata":{"id":"GcHxq5Nm5-jy"},"id":"GcHxq5Nm5-jy","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Find the total number of null values per column using the `isna` method:"],"metadata":{"id":"FU8FYX9j6D3z"},"id":"FU8FYX9j6D3z"},{"cell_type":"code","source":[],"metadata":{"id":"rJTbQqwb5s4S"},"id":"rJTbQqwb5s4S","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After looking more closely, we see that there are null values in some columns. Management doesn't want any null values in the cleaned data. But if we just drop all the rows with null values, we'll lose data unneccessarily. We want to keep any data we can and discard only the records we can't reasonably salvage. \n","\n","Let's start with the `salary` column, which has 14 null values. Can we reasonably infer what those missing salaries are?\n","\n","While it's possible to guess what those missing values could be, the results are not guaranteed to be accurate. This could skew our analysis. Also, the rows with missing salaries have missing job titles too, which we would want to know as well. Since there are only 14 rows (~2% of the total data) that are affected, the best approach for our purposes would be to just drop them.\n","\n","Drop the rows with null values in the `salary` column: "],"metadata":{"id":"2FpTOkYvFFjs"},"id":"2FpTOkYvFFjs"},{"cell_type":"code","source":[],"metadata":{"id":"S6C_03UCH7st"},"id":"S6C_03UCH7st","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now verify that the rows in the salary column were dropped by checking the null values in `ds_salaries_clean` again:"],"metadata":{"id":"90MpY7VoKJ8Z"},"id":"90MpY7VoKJ8Z"},{"cell_type":"code","source":[],"metadata":{"id":"sdCCjE0pKfOx"},"id":"sdCCjE0pKfOx","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Test\n","Run the following cell to ensure that you dropped the rows correctly before continuing (There should be no output if correct):"],"metadata":{"id":"Qmb-fInZb7Uf"},"id":"Qmb-fInZb7Uf"},{"cell_type":"code","source":["assert ds_salaries_clean.salary.isna().sum() == 0"],"metadata":{"id":"Rz3Q_8-icBEt"},"id":"Rz3Q_8-icBEt","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that we've taken care of removing those rows, let's look at the other columns with null values. The `company_size` column has 8 null values. This is another case where we could possibly guess at these values, but for our purposes its better to drop rows where this is null.\n","\n","Go ahead and drop all rows where `company_size` is null:"],"metadata":{"id":"ywL7YvbiOU8c"},"id":"ywL7YvbiOU8c"},{"cell_type":"code","source":[],"metadata":{"id":"uRvUOh0oPHoj"},"id":"uRvUOh0oPHoj","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Test\n","\n","Run the following code cell to check your work:"],"metadata":{"id":"mfNjY9jgcblw"},"id":"mfNjY9jgcblw"},{"cell_type":"code","source":["assert ds_salaries_clean.company_size.isna().sum() == 0"],"metadata":{"id":"aAhiHFGLceNz"},"id":"aAhiHFGLceNz","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We've handled most of the null values in our data so far. The last column that we have to deal with is `remote_work_type`, which has 41 nulls. If we look at both `remote_work_type` and `remote_ratio`, we should see that `remote_work_type` is directly related to the `remote_ratio`. We can determine what the work type is by using the ratio. Since we have all the ratios and are only missing some types, we can fill in the nulls using the `remote_ratio`. \n","\n","Use the example below, which fills in the values for `On-site only` types, to fill in the nulls for `Hybrid` and `Fully remote`: "],"metadata":{"id":"fG8RVPdFPgYW"},"id":"fG8RVPdFPgYW"},{"cell_type":"markdown","source":["Run the next cell:"],"metadata":{"id":"xTFepvKWAR-y"},"id":"xTFepvKWAR-y"},{"cell_type":"code","source":["# Create a filter for \"On-site only\" remote ratios\n","onsite_only = ds_salaries_clean.loc[ds_salaries_clean.remote_ratio == 0]\n","# Set the `remote_work_type` values for the data found by the filter to \"On-site only\"\n","ds_salaries_clean.loc[onsite_only.index, 'remote_work_type'] = 'On-site only'"],"metadata":{"id":"nFbxFaaDQv4o"},"id":"nFbxFaaDQv4o","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Your Answer here"],"metadata":{"id":"1sl46zTNTAcT"},"id":"1sl46zTNTAcT","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Test\n","\n","After filling in the missing values for `remote_work_type`, there should be no more null values in the data. Run the cell below to confirm that your work is correct so far:"],"metadata":{"id":"rGkRUwg4SnKd"},"id":"rGkRUwg4SnKd"},{"cell_type":"code","source":["# There are no nulls in remote_work_type\n","assert ds_salaries_clean.remote_work_type.isna().sum() == 0\n","# The values in remote_work_type were created correctly\n","values = list(ds_salaries_clean.remote_work_type.unique())\n","values_check = ['On-site only', 'Fully remote', 'Hybrid']\n","assert  len(values) == len(values_check) and len([i for i in values if i in values_check]) == 3\n","# There are no more null values in ds_salaries_clean\n","assert ds_salaries_clean.isna().sum().sum() == 0"],"metadata":{"id":"4lbGvIJpS592"},"id":"4lbGvIJpS592","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"062dbe6c","metadata":{"id":"062dbe6c"},"source":["Great work! \n","\n","The nulls are taken care of, but there is an issue with the `salary` column - these salaries are all in different currencies! Good thing we have the `usd_exchange_rates` table. We can use that data to convert all the `salary` information in `ds_salaries_clean` to USD for consistency.\n","\n","First, familiarize yourself with `usd_exchange_rates` by looking at the data and checking the column definitions in the Data Dictionary supplied by management:"]},{"cell_type":"code","source":[],"metadata":{"id":"Rkbu9sn6XlPl"},"id":"Rkbu9sn6XlPl","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that you're familiar with the `usd_exchange_rates`, it's time to join it to `ds_salaries_clean`. Fill in the code below to join the two tables:"],"metadata":{"id":"08vM81XDXlg7"},"id":"08vM81XDXlg7"},{"cell_type":"code","source":["ds_salaries_clean = pd.function_to_join(\n","    ds_salaries_clean, \n","    usd_exchange_rates,\n","    left_on = [\"Columns to join from ds_salaries_clean\"],\n","    right_on = [\"Columns to join from usd_exchange_rates\"]    \n",")"],"metadata":{"id":"89xX3x1eYF4f"},"id":"89xX3x1eYF4f","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["With our data merged, it's time to convert the salary. Create a new column called `salary_in_usd` that is the result of the `salary` divided by `exchange_rate`. \n","\n","Example:\n","\n","```python\n","# Note: The `round` function can be used to round your results\n","df.loc[:, 'new_column'] = round(df['col1'] / df['col2'])\n","```"],"metadata":{"id":"9VqAXuGjZE6N"},"id":"9VqAXuGjZE6N"},{"cell_type":"code","source":[],"metadata":{"id":"jvT801YlSfC6"},"id":"jvT801YlSfC6","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"9b3f2a0b","metadata":{"id":"9b3f2a0b"},"source":["Great! Now we have a uniform way to measure the salary information.\n","\n","Before we finish cleaning, there are a few columns with abbreviations (`experience_level`, `employment_type`, and `company_location`). We should have the full names of these terms as columns in the cleaned data set.\n","\n","Using the data dictionary and the `merge` function, join the `experience_levels`, `employment_types`, and `countries` data into `ds_salaries_clean` to translate the abbreviations:\n","\n","Example:\n","```python\n","df = pd.merge(df, df2, left_on=\"df_col\", right_on=\"df2_col\")\n","```"]},{"cell_type":"code","source":[],"metadata":{"id":"-fQUFDhCuKNF"},"id":"-fQUFDhCuKNF","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"2557f18a","metadata":{"id":"2557f18a"},"source":["Ok, now we're almost done cleaning. But if we look at `ds_salaries_clean`, there are some extra columns we don't need that were added from the joins we just did. \n","\n","Drop the following columns: \n","\n","`abbreviation_x`,`abbreviation_y`, `abbreviation`, `iso_code`, `ref_date` "]},{"cell_type":"code","source":[],"metadata":{"id":"0BZOMNs6vEwo"},"id":"0BZOMNs6vEwo","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Test\n","\n","Run the cell below to confirm the columns were dropped correctly:"],"metadata":{"id":"6ug-gnow4uNz"},"id":"6ug-gnow4uNz"},{"cell_type":"code","source":["dropped_cols = ['abbreviation_x','abbreviation_y', 'abbreviation', 'iso_code', 'ref_date']\n","assert len([i for i in ds_salaries_clean if i in dropped_cols]) == 0"],"metadata":{"id":"TgOl27yJ4uUR"},"id":"TgOl27yJ4uUR","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We've handled all the nulls, added columns with more information, and mostly cleaned our data. The only thing that's left is to remove any duplicate rows. Go ahead and drop the duplicates using the `drop_duplicates` method on `ds_salaries_clean`: "],"metadata":{"id":"zB0FQlmS3tdi"},"id":"zB0FQlmS3tdi"},{"cell_type":"code","source":[],"metadata":{"id":"s3I0Tj8A4GCI"},"id":"s3I0Tj8A4GCI","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"d1ce3445","metadata":{"id":"d1ce3445"},"source":["Great work!\n","\n","Now the data should be clean and ready for analysis! Run the next cell as one last test before writing the new table as a `csv` and starting your analysis:\n","\n","\n","#### Test"]},{"cell_type":"code","execution_count":null,"id":"9c2860ea","metadata":{"id":"9c2860ea"},"outputs":[],"source":["# Test for null values\n","assert ds_salaries_clean.isna().sum().sum() == 0\n","# Test for duplicates\n","assert ds_salaries_clean.duplicated().sum() == 0"]},{"cell_type":"markdown","id":"65efa406","metadata":{"id":"65efa406"},"source":["If the tests in the cell above pass, you're ready to do your analysis! Let's write the cleaned table to a csv file first:"]},{"cell_type":"code","execution_count":null,"id":"6069d653","metadata":{"id":"6069d653"},"outputs":[],"source":["\"\"\"\n","This line is meant to write their `clean` dataframe to a csv file for submission. This will save\n","the file in the same folder where this notebook is located. The file name will be `ds_salaries_clean.csv`.\n","\n","For students using Google Colab, the file will be saved to the folder space accessed via the menu on the\n","left side of the screen. They can download the file to their local machine from there.  \n","\"\"\"\n","\n","ds_salaries_clean.to_csv(\"ds_salaries_clean.csv\", index=False)"]},{"cell_type":"markdown","id":"59f60373","metadata":{"id":"59f60373"},"source":["## Part Two: Analysis\n","\n","Now it's time to start analyzing the data and answering the questions from management. If you haven't done so yet, familiarize yourself with the questions and data dictionary to get a better sense of the data. \n","\n","The questions asked mostly involve summarizing parts of the data and reporting your findings. You will be doing a lot of aggregate functions (get your `groupby` ready!)\n","\n","Unlike the previous section, this one is not as guided. You will get some hints, but you are expected to answer the questions using your existing `Python` and/or `SQL` knowledge along with help from Google and the other resources provided. \n","\n","You can use `SQL` to query the data to get the answers, or you can use `Python` only, or a combination of both.\n","\n","There are only two rules:\n","\n","1. You must create at least 3 visuals using `matplotlib`\n","2. If you answer a question using `SQL` only, still create a new `pandas` DataFrame that has the results of your query\n","\n"," Example: \n"," ```python\n"," # Create a new DataFrame from a SQL query\n"," my_query = \"\"\"\n","      SELECT * \n","      FROM ds_salaries\n"," \"\"\"\n"," new_df = pd.read_sql(my_query, con=engine)\n"," ```\n","\n","Now on to the questions!\n","\n","\n","*Note: There is a table in the database called `ds_salaries_clean` that is a copy of the cleaned table that you should use for your queries if you're using `SQL`* "]},{"cell_type":"markdown","id":"436f8534","metadata":{"id":"436f8534"},"source":["### What is the average overall salary (regardless of year) by job category?\n","\n",">*Hint*:\n",">\n",">*One of the keywords in this question is `by`. When you see this, it indicates that you'll need to summarize the data by the field(s) that comes after `by`. You'll have to use `groupby` and an aggregate function to solve this.*\n",">\n",">*Make sure to order the results so that it's easy to see the largest salaries vs the smallest. This way answers are clearly visible.* \n",">\n",">*Also, use the `round` function to format your salary results to make them look neater!*\n"]},{"cell_type":"code","source":[" "],"metadata":{"id":"xNUvTUTeI66b"},"id":"xNUvTUTeI66b","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"7f4e9494","metadata":{"id":"7f4e9494"},"source":["### What is the average salary by job category and experience level (regardless of year)?\n","\n",">*Hint*:\n",">\n",">*This is similar to the first question, except instead of grouping by one column, you're grouping by two. Order your results by `job_category`*\n",">\n",">*Also, use the `round` function to format your salary results to make them look neater!*\n",">\n",">*This will be tough to make a visual for since we didn't go over subplots. If you want to challenge yourself, go ahead and look up tutorials on how to do this! But be mindful of time and consider using a table to visualize this* "]},{"cell_type":"code","source":[],"metadata":{"id":"fgS33JwYBQbA"},"id":"fgS33JwYBQbA","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"6af276a4","metadata":{"id":"6af276a4"},"source":["### In which country do data analysts make the most money on average? Which country do they make the least?\n","\n",">*Hint*:\n",">\n",">*This is a filtering and aggregation problem. While we don't have the `by` keyword here, the question asks us to look at average salary for data analysts in each country. We are  filtering for data analysts and grouping salaries by country.*\n",">\n",">*Filter the data by `Data Analyst` and then get the average salary by country for that subset. Finally, we want to see the min and max values for this subset. Solving to get just the min and max values is a bit more complex than what we've learned so far. It is OK to show the entire list (ordered, of course) and highlight the highest and lowest values in your response.*"]},{"cell_type":"code","source":[],"metadata":{"id":"o9ZJh1WsBRF_"},"id":"o9ZJh1WsBRF_","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"bff295c5","metadata":{"id":"bff295c5"},"source":["### Which job category earns more on average: Data Analyst, Data Science, or Data Engineering?\n","\n",">*Hint*:\n",">\n",">*Another filtering and aggregation problem. This time we're looking at average salary by specific job categories. Use the same pattern as the previous filtering/aggregation questions that we answered to complete this.*"]},{"cell_type":"code","source":[],"metadata":{"id":"YZtLVxa7BRqZ"},"id":"YZtLVxa7BRqZ","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"bedc2b7a","metadata":{"id":"bedc2b7a"},"source":["### Which company size pays data professionals the most on average? Is there a relationship between company size and average pay?\n","\n",">*Hint:*\n",">\n",">*This is an aggregation problem with an additional question that can only be answered following the aggregation. Follow the same pattern as the previous questions and use the results to determine whether there is a relationship between pay and company size.* "]},{"cell_type":"code","source":[],"metadata":{"id":"LDn9R6PSn1a6"},"id":"LDn9R6PSn1a6","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"9ac2f48f","metadata":{"id":"9ac2f48f"},"source":["### Which job category pays the most on average in 2022?\n","\n",">*Hint:*\n",">\n",">*This is another filtering and aggregation problem. Apply the same pattern you've used previously to solve this. You should return only the job category with the highest average pay in 2022* "]},{"cell_type":"code","source":[],"metadata":{"id":"UtmD_V4KsPVP"},"id":"UtmD_V4KsPVP","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"91a48b66","metadata":{"id":"91a48b66"},"source":["### How are companies working in 2022 (Remote, In-Office, Hybrid)?\n","\n",">*Hint:*\n",">\n",">*This is also a filtering and aggregation problem (a fairly common theme.) Apply the same pattern you've used previously to solve this.* "]},{"cell_type":"code","source":[],"metadata":{"id":"CEviGLvjvN9o"},"id":"CEviGLvjvN9o","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"f6345826","metadata":{"id":"f6345826"},"source":["### What additional analysis/insights can you make using the dataset?\n","\n",">*Hint:*\n",">\n",">*Here's your chance to provide management with unique input or insight based on what you have found in the database to management.* \n",">\n",">*This is an optional question, but we encourage you to explore the data for relationships or patterns that were not asked about but may be relevant.* "]},{"cell_type":"code","execution_count":null,"id":"562baa69","metadata":{"id":"562baa69"},"outputs":[],"source":["\"\"\"\n","This is an optional and totally open ended question. It's not necessarily meant to be graded but rather\n","to challenge students to think beyond the outline we gave them\n","\"\"\""]},{"cell_type":"markdown","id":"40a52092","metadata":{"id":"40a52092"},"source":["## Part Three: Delivering Results\n","\n","You've cleaned the data, answered your manager's questions, and created some visualizations to share your insights. Job well done! Now its time to send the results to your manager. \n","\n","Compose an email response to your manager's request. Provide an overview of the analysis you did and the materials you are submitting for their review. Attach a copy of the `ds_salaries_clean` data set, as well as this notebook, to the email and send it to your pod captain."]},{"cell_type":"code","source":[],"metadata":{"id":"J3H1bo7mDWN3"},"id":"J3H1bo7mDWN3","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"colab":{"provenance":[{"file_id":"1sJrgAde8pqnM6sjaNbexD897oor5Q9T9","timestamp":1666732446796},{"file_id":"1yid4jUgNE1AUL1JZChW7h9QqQyYH6oUe","timestamp":1666010861264},{"file_id":"1GTuLCEVkJpqLBVLp5zQ-fhIX_Uyi8EyG","timestamp":1665860390038}]}},"nbformat":4,"nbformat_minor":5}